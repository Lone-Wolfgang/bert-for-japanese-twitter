{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from process_twarc.util import concat_dataset, get_all_files\n",
    "from modules.build_corpus import build_chunked_corpus \n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paths to inputs and outputs.\n",
    "paths = {\n",
    "    \"data\": \"../../data/tweets/3-tokenized\", #change this path to the Tweets folder\n",
    "    \"corpus_output\": \"process\", # Make a path to the folder where you want to save your chunked subcorpus. It will generate the folder for you if it doens't exist.\n",
    "}\n",
    "input_dir, output_dir = paths.values()\n",
    "\n",
    "#To pass to build_chunked_corpus. Let's build a corpus with 0.1% of the dataset, 1 time, with the corpus divided into 10 chunks.\n",
    "parameters = {\n",
    "    \"sample_frac\": 0.001,\n",
    "    \"num_epochs\": 1,\n",
    "    \"num_chunks\": 10\n",
    "}\n",
    "sample_frac, num_epochs, num_chunks = parameters.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excecute the procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_chunked_corpus(\n",
    "    data_dir= input_dir,\n",
    "    output_dir= output_dir,\n",
    "    sample_frac=sample_frac,\n",
    "    num_epochs=num_epochs,\n",
    "    num_chunks=num_chunks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrate that an error occured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the dataset used to construct the subcorpus, generate a set of all the tweet_ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = get_all_files(input_dir) #Get all the filepaths from the input dir.\n",
    "\n",
    "#Concatenate a single column dataset with just the tweet_ids\n",
    "dataset = concat_dataset(\n",
    "    file_paths=file_paths,\n",
    "    output_type=\"Dataset\",\n",
    "    columns=\"tweet_id\"\n",
    ")\n",
    "\n",
    "all_ids = set(dataset[\"tweet_id\"])\n",
    "print()\n",
    "print(f\"Total Tweets: {len(all_ids)}\")\n",
    "print(f\"Sample: {list(all_ids)[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the chunked subcorpus for tweet_ids that are absent from all_ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks(output_dir):\n",
    "    return os.listdir(output_dir)\n",
    "\n",
    "def load_subcorpus (chunk):\n",
    "    return pd.read_json(f\"{output_dir}/{chunk}/subcorpus.jsonl\", lines = True, encoding=\"utf-8\")\n",
    "\n",
    "def check_tweet_ids(subcorpus):\n",
    "    output_ids = set(subcorpus[\"tweet_id\"].astype(\"str\").to_list())\n",
    "    error_ids = output_ids.difference(all_ids)\n",
    "\n",
    "    print()\n",
    "    print(\"Total Tweets:\", len(output_ids))\n",
    "    print(\"Copied with error:\", len(error_ids))\n",
    "    print(\"Error Ratio: {:.2%}\".format(len(error_ids)/len(all_ids)*100))\n",
    "    print(\"Sample of Erroneous IDs\", list(error_ids)[:5])\n",
    "    print()\n",
    "    return\n",
    "\n",
    "chunks = get_chunks(output_dir)\n",
    "for chunk in chunks:\n",
    "    print(f\"Checking {chunk}\")\n",
    "    subcorpus = load_subcorpus(chunk)\n",
    "    check_tweet_ids(subcorpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I run the procedure above,  I got more then 40% Error rate.\n",
    "\n",
    "Rather then compiling the tweet_ids from all flagged duplicates, I compiled the text.\n",
    "\n",
    "I suspsect that there were errors in copying over the text as well. However, as most duplicates are identical rather than near,\n",
    "compiling text was still effective for filtering.\n",
    "\n",
    "So, at the moment, this particular issue is not a high priority. However, I've got this iron this out before I can considered\n",
    "publishing this code."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
