{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from process_twarc.util import load_parquet\n",
    "from modules.build_corpus import build_chunked_corpus\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Define the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paths to inputs and outputs.\n",
    "paths = {\n",
    "    \"data\": \"../../data/tweets/3-tokenized\",\n",
    "    \"corpus_output\": \"process\",\n",
    "    \"tweet_ids\": \"../../data/corpus-analysis/ids/tweet_ids.parquet\"\n",
    "}\n",
    "input_dir, output_dir, path_to_tweet_ids = paths.values()\n",
    "\n",
    "#To pass to build_chunked_corpus. Let's build a corpus with 0.1% of the dataset, 1 time, with the corpus divided into 10 chunks.\n",
    "parameters = {\n",
    "    \"sample_frac\": 0.001,\n",
    "    \"num_epochs\": 1,\n",
    "    \"num_chunks\": 10\n",
    "}\n",
    "sample_frac, num_epochs, num_chunks = parameters.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Excecute the procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset:   0%|                                                                         | 0/343 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|███████████████████████████████████████████████████████████████| 343/343 [00:27<00:00, 12.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating epoch 1\n",
      "\n",
      "Sampling 59570 tweets.\n",
      "Corpus compiled and shuffled.\n",
      "Generated folder for chunk001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building subcorpus: 100%|███████████████████████████████████████████████████████| 5958/5958 [00:00<00:00, 13594.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated folder for chunk002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building subcorpus: 100%|███████████████████████████████████████████████████████| 5958/5958 [00:00<00:00, 10777.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated folder for chunk003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building subcorpus: 100%|███████████████████████████████████████████████████████| 5958/5958 [00:00<00:00, 10471.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated folder for chunk004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building subcorpus: 100%|███████████████████████████████████████████████████████| 5958/5958 [00:00<00:00, 12000.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated folder for chunk005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building subcorpus: 100%|███████████████████████████████████████████████████████| 5958/5958 [00:00<00:00, 12623.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated folder for chunk006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building subcorpus: 100%|███████████████████████████████████████████████████████| 5958/5958 [00:00<00:00, 12948.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated folder for chunk007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building subcorpus: 100%|███████████████████████████████████████████████████████| 5958/5958 [00:00<00:00, 13313.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated folder for chunk008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building subcorpus: 100%|███████████████████████████████████████████████████████| 5958/5958 [00:00<00:00, 11774.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated folder for chunk009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building subcorpus: 100%|███████████████████████████████████████████████████████| 5958/5958 [00:00<00:00, 12464.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated folder for chunk010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building subcorpus: 100%|███████████████████████████████████████████████████████| 5948/5948 [00:00<00:00, 12696.92it/s]\n"
     ]
    }
   ],
   "source": [
    "build_chunked_corpus(\n",
    "    data_directory = input_dir,\n",
    "    save_directory = output_dir,\n",
    "    sample_frac=sample_frac,\n",
    "    num_epochs=num_epochs,\n",
    "    num_chunks=num_chunks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Demonstrate that an error occured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Load the Tweet IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet IDs loaded.\n",
      "Total IDs: 51116358\n"
     ]
    }
   ],
   "source": [
    "def load_tweet_ids (path):\n",
    "    return set(load_parquet(path)[\"tweet_id\"])\n",
    "\n",
    "all_ids = load_tweet_ids(path_to_tweet_ids)\n",
    "\n",
    "print(\"Tweet IDs loaded.\")\n",
    "print(\"Total IDs:\", len(all_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Check the chunked subcorpus for tweet_ids that are absent from all_ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking chunk001\n",
      "\n",
      "Total Tweets: 5958\n",
      "Copied with error: 3201\n",
      "Error Ratio: 0.63%\n",
      "\n",
      "Checking chunk002\n",
      "\n",
      "Total Tweets: 5958\n",
      "Copied with error: 3132\n",
      "Error Ratio: 0.61%\n",
      "\n",
      "Checking chunk003\n",
      "\n",
      "Total Tweets: 5958\n",
      "Copied with error: 3225\n",
      "Error Ratio: 0.63%\n",
      "\n",
      "Checking chunk004\n",
      "\n",
      "Total Tweets: 5958\n",
      "Copied with error: 3206\n",
      "Error Ratio: 0.63%\n",
      "\n",
      "Checking chunk005\n",
      "\n",
      "Total Tweets: 5958\n",
      "Copied with error: 3174\n",
      "Error Ratio: 0.62%\n",
      "\n",
      "Checking chunk006\n",
      "\n",
      "Total Tweets: 5958\n",
      "Copied with error: 3205\n",
      "Error Ratio: 0.63%\n",
      "\n",
      "Checking chunk007\n",
      "\n",
      "Total Tweets: 5958\n",
      "Copied with error: 3170\n",
      "Error Ratio: 0.62%\n",
      "\n",
      "Checking chunk008\n",
      "\n",
      "Total Tweets: 5958\n",
      "Copied with error: 3219\n",
      "Error Ratio: 0.63%\n",
      "\n",
      "Checking chunk009\n",
      "\n",
      "Total Tweets: 5958\n",
      "Copied with error: 3183\n",
      "Error Ratio: 0.62%\n",
      "\n",
      "Checking chunk010\n",
      "\n",
      "Total Tweets: 5948\n",
      "Copied with error: 3223\n",
      "Error Ratio: 0.63%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_chunks(output_dir):\n",
    "    return os.listdir(output_dir)\n",
    "\n",
    "def load_subcorpus (chunk):\n",
    "    return pd.read_json(f\"{output_dir}/{chunk}/subcorpus.jsonl\", lines = True, encoding=\"utf-8\")\n",
    "\n",
    "def check_tweet_ids(subcorpus):\n",
    "    output_ids = set(subcorpus[\"tweet_id\"].astype(\"str\").to_list())\n",
    "    error_ids = output_ids.difference(all_ids)\n",
    "\n",
    "    print()\n",
    "    print(\"Total Tweets:\", len(output_ids))\n",
    "    print(\"Copied with error:\", len(error_ids))\n",
    "    print(\"Error Ratio: {:.2%}\".format(len(error_ids)/len(all_ids)*100))\n",
    "    print()\n",
    "    return\n",
    "\n",
    "chunks = get_chunks(output_dir)\n",
    "for chunk in chunks:\n",
    "    print(f\"Checking {chunk}\")\n",
    "    subcorpus = load_subcorpus(chunk)\n",
    "    check_tweet_ids(subcorpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I run the procedure above,  I got almost 2/3 Error rate.\n",
    "\n",
    "Rather then compiling the tweet_ids from all flagged duplicates, I compiled the text.\n",
    "\n",
    "I suspsect that there were errors in copying over the text as well. However, as most duplicates are identical rather than near,\n",
    "compiling text was still effective for filtering.\n",
    "\n",
    "So, at the moment, this particular issue is not a high priority. However, I've got this iron this out before I can considered\n",
    "publishing this code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
