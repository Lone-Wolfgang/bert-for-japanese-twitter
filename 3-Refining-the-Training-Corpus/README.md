# Refining the Training Corpus

<div align="justify">

The evaluation of the 60 million tweet corpus revealed two major issues: repetitive text and user imbalance. This chapter outlines the steps taken to refine the raw corpus into one that is leaner and more linguistically diverse. It begins by discussing how duplicate text undermines training. Next, it describes two methods that were employed to refine the corpus. First, an algorithm involving n-grams was used to remove near-duplicates, a process that was both time-consuming and computationally intensive. This section defines the algorithm, details parameter fine-tuning, and outlines the deduplication procedure. The impact of deduplication is discussed, illustrated by text samples from varyingly affected  users. Further refinement is achieved by balancing authors’ contributions with a simple heuristic. The raw and refined corpora are evaluated by size, user balance, and frequency distribution of n-grams, hashtags, and mentions.

</div>

## Pitfalls of Duplicate Text

<div align="justify">

On of the challenges of working with web corpora is the ubiquity of duplicate text. Duplicates are problematic for training. First, they can cause data leakage, where the same examples are used in both the training and the test set. Excessive data leakage produces models that memorize rather than generalize (Elangovan et al., 2021). Such models tend to perform well on their test set but struggle outside of the training context.  The other consideration is efficiency. Repeated observation of similar token sequences is wasted training time. Deduplicating corpora speeds up convergence by reducing the training load while preserving language variety. If removing duplicates resolves some data quality issues, there is potential to improve performance (Lee et al., 2022).

</div>

## Addressing Template Generated Text

<div align="justify">

Chapter 4 raises concerns about the 5 million tweets that were generated by Swarm. These tweets use an easily identifiable template: “I’m at {PLACE} in {PLACE}”. Template-generated text creates strong, unnatural associations between the tokens used in the template, necessitating their removal. The simplest deduplication method is to remove identical examples, but this is insufficient for addressing template-generated text. For example, with exact duplicates removed, 1.6 million of the 5 million Swarm generated tweets remain. Once the template has been identified, regular expressions prove highly effective at targeting such text. However, the template problem extends beyond Swarm. Identification of all templates scattered throughout the corpus is beyond the resources of this project. Therefore, a more sophisticated, data-driven approach was required.

</div>

## Efficient Near-Duplicate Deduplication Using Min-Hashes

<div align="justify">

Near-deduplication involves reducing texts that exceed a certain similarity threshold. A common approach segments text into n-gram shingles to compute the Jaccard similarity coefficient, as demonstrated in Appendix 1. This method is highly effective for identifying template generated texts but poses scalability challenges. For example, the 60 million tweet corpus entails over 100 trillion comparisons. To address resource constraints, shingled texts are further processed into Min-Hashes. Min-Hashes are preferred because they quickly and accurately estimate similarity. Furthermore, they cluster well, which substantially reduces the complexity of the problem (Broder, 1997; Lee et al., 2022). The specially designed Python library, NLPDedup , was used to carry out the algorithm. The deduplication procedures were performed on a server with an 8-core CPU and 62.6Gb of RAM.

</div>

## Fine-Tuning the Deduplication Parameters

<div align="justify">

The deduplication algorithm is controlled by several parameters that impact its sensitivity, accuracy, and complexity. Two parameters that control sensitivity—Jaccard similarity and n-gram size—were chosen for fine-tuning to optimize the process: Jaccard similarity is a  ratio that  indicates the percentage of shared n-gram sequences required for two tweets to be considered nearly identical. Higher thresholds result in fewer duplicates being recognized, thus reducing sensitivity. N-gram size represents the number of tokens in each n-gram shingle. Larger n-grams decrease the likelihood of shared sequences, further reducing sensitivity. To determine the optimal configuration for deduplication, two sample corpora were prepared:

- **Target Corpus**: Consists of 5 million tweets generated by Swarm. This set acts as a benchmark for measuring recall. The Swarm tweets are simple. To be robust against more complex templates, the configuration must be aggressive enough to eliminate the target.
- **Control Corpus**: Comprises 5 million tweets from the raw corpus, excluding Swarm tweets. This set helps estimate precision. Although the known templates are removed, a significant portion is still duplicate. The ideal trial will flag close to 100% of the target while showing restraint with the control.

Deduplication was applied to both corpora under various configurations, with the results displayed in Table 5.1 and Figure 5.1. This approach aims to fine-tune the process, ensuring high efficiency in recognizing and removing duplicates without excessively impacting the integrity of the control corpus.
